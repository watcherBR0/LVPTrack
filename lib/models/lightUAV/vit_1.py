""" Vision Transformer (ViT) in PyTorch
A PyTorch implement of Vision Transformers as described in:
'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'
    - https://arxiv.org/abs/2010.11929
`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`
    - https://arxiv.org/abs/2106.10270
The official jax code is released and available at https://github.com/google-research/vision_transformer
DeiT model defs and weights from https://github.com/facebookresearch/deit,
paper `DeiT: Data-efficient Image Transformers` - https://arxiv.org/abs/2012.12877
Acknowledgments:
* The paper authors for releasing code and weights, thanks!
* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out
for some einops/einsum fun
* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT
* Bert reference code checks against Huggingface Transformers and Tensorflow Bert
Hacked together by / Copyright 2021 Ross Wightman

Modified by Botao Ye
"""
import math
import logging
from functools import partial
from collections import OrderedDict
from copy import deepcopy

import torch
import torch.nn as nn
import torch.nn.functional as F

from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
from timm.models.helpers import build_model_with_cfg, named_apply, adapt_input_conv
from timm.models.layers import Mlp, DropPath, to_2tuple, trunc_normal_, lecun_normal_
from timm.models.registry import register_model
from .utils import combine_tokens, token2feature, feature2token
from lib.models.layers.patch_embed import PatchEmbed



class ConvBlock(torch.nn.Module):
    def __init__(self, input_size, output_size, kernel_size, stride, padding, bias=True, isuseBN=False):
        super(ConvBlock, self).__init__()
        self.isuseBN = isuseBN
        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride, padding, bias=bias)
        if self.isuseBN:
            self.bn = nn.BatchNorm2d(output_size)
        self.act = torch.nn.PReLU()

    def forward(self, x):
        out = self.conv(x)
        if self.isuseBN:
            out = self.bn(out)
        out = self.act(out)
        return out

class DCEBlock(torch.nn.Module):
    def __init__(self, input_size, output_size, kernel_size, stride, padding, bias=True):
        super(DCEBlock, self).__init__()
        codedim=output_size//2
        self.conv_Encoder = ConvBlock(input_size, codedim, 3, 1, 1,isuseBN=False)
        self.conv_Offset = ConvBlock(codedim, codedim, 3, 1, 1,isuseBN=False)
        self.conv_Decoder = ConvBlock(codedim, output_size, 3, 1, 1,isuseBN=False)

    def forward(self, x):
        code= self.conv_Encoder(x)
        offset = self.conv_Offset(code)
        code_lighten = code+offset
        out = self.conv_Decoder(code_lighten)
        return out

class DCUBlock(torch.nn.Module):
    def __init__(self, input_size, output_size, kernel_size, stride, padding, bias=True):
        super(DCUBlock, self).__init__()
        codedim=output_size//2
        self.conv_Encoder = ConvBlock(input_size, codedim, 3, 1, 1,isuseBN=False)
        self.conv_Offset = ConvBlock(codedim, codedim, 3, 1, 1,isuseBN=False)
        self.conv_Decoder = ConvBlock(codedim, output_size, 3, 1, 1,isuseBN=False)

    def forward(self, x):
        code= self.conv_Encoder(x)
        offset = self.conv_Offset(code)
        code_lighten = code-offset
        out = self.conv_Decoder(code_lighten)
        return out

class FusionLayer(nn.Module):
    def __init__(self, inchannel, outchannel, reduction=16):
        super(FusionLayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(inchannel, inchannel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(inchannel // reduction, inchannel, bias=False),
            nn.Sigmoid()
        )
        self.outlayer = ConvBlock(inchannel, outchannel, 1, 1, 0, bias=True)

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        y = x * y.expand_as(x)
        y = y + x
        y = self.outlayer(y)
        return y


class LBP(torch.nn.Module):
    def __init__(self, input_size, hide_channel, output_size, kernel_size, stride, padding):
        super(LBP, self).__init__()
        self.conv0 = nn.Conv2d(in_channels=input_size,out_channels=hide_channel, kernel_size=1, stride=1, padding=0)
        self.fusion = FusionLayer(hide_channel,hide_channel)
        self.conv1_1 = DCEBlock(hide_channel, hide_channel, kernel_size, stride, padding, bias=True)
        self.conv2 = DCUBlock(hide_channel, hide_channel, kernel_size, stride, padding, bias=True)
        self.conv3 = DCEBlock(hide_channel, hide_channel, kernel_size, stride, padding, bias=True)
        self.local_weight1_1 = ConvBlock(hide_channel, hide_channel, kernel_size=1, stride=1, padding=0, bias=True)
        self.local_weight2_1 = ConvBlock(hide_channel, hide_channel, kernel_size=1, stride=1, padding=0, bias=True)
        self.conv4 = nn.Conv2d(in_channels=hide_channel,out_channels=output_size, kernel_size=1, stride=1, padding=0)

    #公式6
    def forward(self, x):
        x = self.conv0(x)
        x=self.fusion(x)
        hr = self.conv1_1(x)
        lr = self.conv2(hr)
        residue = self.local_weight1_1(x) - lr
        h_residue = self.conv3(residue)
        hr_weight = self.local_weight2_1(hr)
        return self.conv4(hr_weight + h_residue)

class Gate_Feature(nn.Module):
    def __init__(self, NUM = 256, GATE_INIT = 10, NUM_TOKENS = 256):
        super().__init__()
        self.num = NUM
        gate_logit = (torch.ones(NUM) * GATE_INIT)
        self.num_tokens = NUM_TOKENS
        self.gate_logit = nn.Parameter(gate_logit)
    def forward(self,xin,xout):
        gate = self.gate_logit.sigmoid()
        gate = gate.unsqueeze(0).unsqueeze(-1).repeat(xin.size(0),1,xin.size(2))
        # convex combinate input and output prompt representations of current block via learnable gate
        prompt_out = gate * xout
        prompt_in = 1 * xin
        xout = prompt_out + prompt_in
        return xout


class Gate_Prompt(nn.Module):
    def __init__(self, NUM = 1, GATE_INIT = 10, NUM_TOKENS = 256):
        super().__init__()
        gate_logit = -(torch.ones(NUM) * GATE_INIT)
        self.num_tokens = NUM_TOKENS
        self.gate_logit = nn.Parameter(gate_logit)
    def forward(self,xin,xout):
        gate = self.gate_logit.sigmoid()
        prompt_in = xin
        # current block's output prompt representation
        prompt_out = xout
        # convex combinate input and output prompt representations of current block via learnable gate
        #论文公式8
        xout = (1 - gate) * prompt_out + gate * prompt_in
        return xout

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, len_t, return_attention=False):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q_s = q[:, :, len_t:]
        attn = (q_s @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x_t = x[:, :len_t]
        x_s = (attn @ v).transpose(1, 2).reshape(B, N-len_t, C)
        x = torch.cat((x_t, x_s), dim=1)
        x = self.proj(x)
        x = self.proj_drop(x)

        if return_attention:
            return x, attn
        return x


class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, attention, len_t=0,):
        if attention == 'lite':
            x_t = x[:, :len_t]
            x_s = x[:, len_t:]

            y = self.attn(self.norm1(x), len_t)
            y_s = y[:, len_t:]

            x_s = x_s + self.drop_path(y_s)
            x_s = x_s +self.drop_path(self.mlp(self.norm2(x_s)))

            x = torch.cat((x_t, x_s), dim=1)
            return x

            # feat, attn = self.attn(self.norm1(x), True)
            # x = x + self.drop_path(feat)
            # x = x + self.drop_path(self.mlp(self.norm2(x)))
            # return x, attn
        else:
            x = x + self.drop_path(self.attn(self.norm1(x), len_t))
            x = x + self.drop_path(self.mlp(self.norm2(x)))
            return x


class VisionTransformer(nn.Module):
    """ Vision Transformer
    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
        - https://arxiv.org/abs/2010.11929
    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`
        - https://arxiv.org/abs/2012.12877
    """

    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=8,
                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,
                 self_blocks_num=4, cross_blocks_num=4, add_target_token=False, attention='lite',
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,
                 act_layer=None, weight_init='',search_size=None, template_size=None,new_patch_size=None):
        """
        Args:
            img_size (int, tuple): input image size
            patch_size (int, tuple): patch size
            in_chans (int): number of input channels
            num_classes (int): number of classes for classification head
            embed_dim (int): embedding dimension
            depth (int): depth of transformer
            num_heads (int): number of attention heads
            mlp_ratio (int): ratio of mlp hidden dim to embedding dim
            qkv_bias (bool): enable bias for qkv if True
            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set
            distilled (bool): model includes a distillation token and head as in DeiT models
            drop_rate (float): dropout rate
            attn_drop_rate (float): attention dropout rate
            drop_path_rate (float): stochastic depth rate
            embed_layer (nn.Module): patch embedding layer
            norm_layer: (nn.Module): normalization layer
            weight_init: (str): weight init scheme
        """
        super().__init__()
        if isinstance(img_size, tuple):
            self.img_size = img_size
        else:
            self.img_size = to_2tuple(img_size)
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_tokens = 2 if distilled else 1
        norm_layer = norm_layer
        act_layer = act_layer or nn.GELU

        self.patch_embed = embed_layer(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        #0704
        # self.patch_embed_extreme = embed_layer(
        #     img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)


        #add by siyuan 0811

        '''patch_embed_prompt'''
        self.patch_embed_prompt = embed_layer(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        '''
        prompt parameters
        '''
        H, W = search_size
        new_P_H, new_P_W = H // new_patch_size, W // new_patch_size
        self.num_patches_search = new_P_H * new_P_W
        H, W = template_size
        new_P_H, new_P_W = H // new_patch_size, W // new_patch_size
        self.num_patches_template = new_P_H * new_P_W
        """add here, no need use backbone.finetune_track """
        self.pos_embed_z = nn.Parameter(torch.zeros(1, self.num_patches_template, embed_dim))
        self.pos_embed_x = nn.Parameter(torch.zeros(1, self.num_patches_search, embed_dim))

        '''prompt parameters'''
        prompt_blocks = []
        block_nums = depth
        for i in range(block_nums):
            prompt_blocks.append(LBP(input_size=768, hide_channel=64, output_size=768, kernel_size=3, stride=1, padding=1))
        self.prompt_blocks = nn.Sequential(*prompt_blocks)

        prompt_norms = []
        for i in range(block_nums):
            prompt_norms.append(norm_layer(embed_dim))
        self.prompt_norms = nn.Sequential(*prompt_norms)

        prompt_gates = []
        for i in range(block_nums-1):
            prompt_gates.append(Gate_Prompt(GATE_INIT = 10, NUM_TOKENS = 256))
        self.prompt_gates = nn.Sequential(*prompt_gates)

        prompt_feature_gates = []
        for i in range(block_nums):
            prompt_feature_gates.append(Gate_Feature(GATE_INIT = 10, NUM_TOKENS = 256))
        self.prompt_feature_gates = nn.Sequential(*prompt_feature_gates)



        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        # self.cls_token_extreme = nn.Parameter(torch.zeros(1, 1, embed_dim))
        # self.dist_token_extreme = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None
        # self.pos_embed_extreme = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))
        # self.pos_drop_extreme = nn.Dropout(p=drop_rate)

        self.self_blocks_num = self_blocks_num
        self.cross_blocks_num = cross_blocks_num

        self.patch_size = patch_size

        self.add_target_token = add_target_token
        self.attention = attention

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        
        self_blocks = []
        for i in range(self_blocks_num):
            self_blocks.append(Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
            ))
        self.self_blocks = nn.ModuleList(self_blocks)

        #0702
        # self_blocks_extreme = []
        # for i in range(self_blocks_num):
        #     self_blocks_extreme.append(Block(
        #         dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
        #         drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,
        #     ))
        # self.self_blocks_extreme = nn.ModuleList(self_blocks_extreme)

        cross_blocks = []
        for i in range(cross_blocks_num):
            cross_blocks.append(Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+self.self_blocks_num], norm_layer=norm_layer,
            ))
        self.cross_blocks = nn.ModuleList(cross_blocks)

        #0702
        # cross_blocks_extreme = []
        # for i in range(cross_blocks_num):
        #     cross_blocks_extreme.append(Block(
        #         dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
        #         drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i+self.self_blocks_num], norm_layer=norm_layer,
        #     ))
        # self.cross_blocks_extreme = nn.ModuleList(cross_blocks_extreme)

        self.norm = norm_layer(embed_dim)
        # self.norm_extreme = norm_layer(embed_dim)

        self.init_weights(weight_init)

    def init_weights(self, mode=''):
        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')
        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.
        trunc_normal_(self.pos_embed, std=.02)
        if self.dist_token is not None:
            trunc_normal_(self.dist_token, std=.02)
        if mode.startswith('jax'):
            # leave cls token as zeros to match jax impl
            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)
        else:
            trunc_normal_(self.cls_token, std=.02)
            self.apply(_init_vit_weights)

    def _init_weights(self, m):
        # this fn left here for compat with downstream users
        _init_vit_weights(m)

    def finetune_track(self, cfg, patch_start_index=1):

        search_size = to_2tuple(cfg.DATA.SEARCH.SIZE)
        template_size = to_2tuple(cfg.DATA.TEMPLATE.SIZE)
        new_patch_size = cfg.MODEL.BACKBONE.STRIDE

        # resize patch embedding
        if new_patch_size != self.patch_size:
            print('Inconsistent Patch Size With The Pretrained Weights, Interpolate The Weight!')
            old_patch_embed = {}
            for name, param in self.patch_embed.named_parameters():
                if 'weight' in name:
                    param = nn.functional.interpolate(param, size=(new_patch_size, new_patch_size),
                                                      mode='bicubic', align_corners=False)
                    param = nn.Parameter(param)
                old_patch_embed[name] = param
            self.patch_embed = PatchEmbed(img_size=self.img_size, patch_size=new_patch_size, in_chans=3,
                                          embed_dim=self.embed_dim)
            self.patch_embed.proj.bias = old_patch_embed['proj.bias']
            self.patch_embed.proj.weight = old_patch_embed['proj.weight']

        # for patch embedding
        patch_pos_embed = self.pos_embed[:, patch_start_index:, :]
        patch_pos_embed = patch_pos_embed.transpose(1, 2)
        B, E, Q = patch_pos_embed.shape
        P_H, P_W = self.img_size[0] // self.patch_size, self.img_size[1] // self.patch_size
        patch_pos_embed = patch_pos_embed.view(B, E, P_H, P_W)

        # for search region
        H, W = search_size
        new_P_H, new_P_W = H // new_patch_size, W // new_patch_size
        search_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H, new_P_W), mode='bicubic',
                                                           align_corners=False)
        search_patch_pos_embed = search_patch_pos_embed.flatten(2).transpose(1, 2)

        # for template region
        H, W = template_size
        new_P_H, new_P_W = H // new_patch_size, W // new_patch_size
        template_patch_pos_embed = nn.functional.interpolate(patch_pos_embed, size=(new_P_H, new_P_W), mode='bicubic',
                                                             align_corners=False)
        template_patch_pos_embed = template_patch_pos_embed.flatten(2).transpose(1, 2)

        self.pos_embed_z = nn.Parameter(template_patch_pos_embed)
        self.pos_embed_x = nn.Parameter(search_patch_pos_embed)

        #0704
        # # for patch embedding
        # patch_pos_embed_extreme = self.pos_embed_extreme[:, patch_start_index:, :]
        # patch_pos_embed_extreme = patch_pos_embed_extreme.transpose(1, 2)
        # B, E, Q = patch_pos_embed_extreme.shape
        # P_H, P_W = self.img_size[0] // self.patch_size, self.img_size[1] // self.patch_size
        # patch_pos_embed_extreme = patch_pos_embed_extreme.view(B, E, P_H, P_W)

        # # for search region
        # H, W = search_size
        # new_P_H, new_P_W = H // new_patch_size, W // new_patch_size
        # search_patch_pos_embed_extreme = nn.functional.interpolate(patch_pos_embed_extreme, size=(new_P_H, new_P_W), mode='bicubic',
        #                                                    align_corners=False)
        # search_patch_pos_embed_extreme = search_patch_pos_embed_extreme.flatten(2).transpose(1, 2)

        # # for template region
        # H, W = template_size
        # new_P_H, new_P_W = H // new_patch_size, W // new_patch_size
        # template_patch_pos_embed_extreme = nn.functional.interpolate(patch_pos_embed_extreme, size=(new_P_H, new_P_W), mode='bicubic',
        #                                                      align_corners=False)
        # template_patch_pos_embed_extreme = template_patch_pos_embed_extreme.flatten(2).transpose(1, 2)

        # self.pos_embed_z_extreme = nn.Parameter(template_patch_pos_embed_extreme)
        # self.pos_embed_x_extreme = nn.Parameter(search_patch_pos_embed_extreme)

    def forward_self(self, tensor, type, attention, target_token=None):
        
        #0707
        # if 'extreme' in loader_type:
        #     tensor = self.patch_embed_extreme(tensor)
        #     if type == 'x':
        #         tensor += self.pos_embed_x_extreme
        #     elif type == 'z':
        #         tensor += self.pos_embed_z_extreme
        #     tensor = self.pos_drop_extreme(tensor)
        # else:
        tensor = self.patch_embed(tensor)
        if type == 'x':
            tensor += self.pos_embed_x
        elif type == 'z':
            tensor += self.pos_embed_z
        tensor = self.pos_drop(tensor)

        if self.add_target_token and type == 'z' and target_token is not None:
            tensor = torch.cat([target_token, tensor], dim=1)

        #0703
        # if 'extreme' in loader_type:
        #     for i, blk in enumerate(self.self_blocks_extreme):
        #         tensor = blk(tensor, attention)
        # else:
        for i, blk in enumerate(self.self_blocks):
            tensor = blk(tensor, attention)

        if self.add_target_token and type == 'z' and target_token is not None:
            tensor = tensor[:,1:]

        return tensor
    
    def forward_cross(self, z, x, attention):
        len_z = self.pos_embed_z.shape[1]

        x = torch.cat((z, x),dim=1)

        #0703
        # if 'extreme' in loader_type:
        #     for i, blk in enumerate(self.cross_blocks_extreme):
        #         x = blk(x, len_t=len_z, attention=attention)
        # else:
        for i, blk in enumerate(self.cross_blocks):
            x = blk(x, len_t=len_z, attention=attention)

        return self.norm(x)

    def forward_cross_prompt(self, z, x, attention, x_prev):
        len_z = self.pos_embed_z.shape[1]

        x = torch.cat((z, x),dim=1)

        #0703
        # if 'extreme' in loader_type:
        #     for i, blk in enumerate(self.cross_blocks_extreme):
        #         x = blk(x, len_t=len_z, attention=attention)
        # else:
        for i, blk in enumerate(self.cross_blocks):
            if i >= 1:
                x_ori = x
                #feature_prev 32*256*768
                feature_prev = x_ori[:, len_z:, :]
                # prompt,只是简单的layernorm
                x = self.prompt_norms[i](x)
                z_tokens = x[:, :len_z, :]
                x_tokens = x[:, len_z:, :]
                z_feat = token2feature(z_tokens)
                x_feat = token2feature(x_tokens)

                #特征层面做prompt enhancement
                z_feat = self.prompt_blocks[i](z_feat)
                x_feat = self.prompt_blocks[i](x_feat)
                z = feature2token(z_feat)
                x = feature2token(x_feat)
                #prompt gates
                #prompt_gates[i - 1]是逐个gate_prompt操作
                x = self.prompt_gates[i-1](x_prev, x)
                x_prev = x
                #feature gates
                x = self.prompt_feature_gates[i](feature_prev, x)
                x = combine_tokens(x_ori[:, :len_z, :], x, mode='direct')

            x = blk(x, len_t=len_z, attention=attention)

        return self.norm(x)

    def forward(self, z, x=None, target_token=None, mode='train', loader_type='normal'):
        if mode == 'z':
            x_feature = self.forward_self(z, type='z', target_token=target_token, attention=self.attention)
        elif mode == 'x':
            x_feature = self.forward_self(x, type='x', target_token=target_token, attention=self.attention)
            x_feature = self.forward_cross(z, x_feature, self.attention)
        elif mode == 'train':
            z_feature = self.forward_self(z, type='z', attention=self.attention, target_token=target_token)
            x_feature = self.forward_self(x, type='x', attention=self.attention)

            # add by siyuan 0711
            z_embed = self.patch_embed(z)
            x_embed = self.patch_embed(x)

            feature_prev = x


            z_feat = token2feature(self.prompt_norms[0](z_embed))
            x_feat = token2feature(self.prompt_norms[0](x_embed))
            z_feat = self.prompt_blocks[0](z_feat)
            x_feat = self.prompt_blocks[0](x_feat)
            # feature2token 将 B*C*H*W 处理成B*HW*C,表示做的prompt增强
            z_dte = feature2token(z_feat)
            x_dte = feature2token(x_feat)

            x_prev = x_dte

            # # feature gates
            # x = self.prompt_feature_gates[0](feature_prev, x_dte)


            # x_feature = self.forward_cross(z_feature, x_feature, self.attention)
            # prompt added by siyuan 0812
            x_feature = self.forward_cross_prompt(z_feature, x_feature, self.attention, x_prev)
        elif mode == 'test':
            z_feature = z
            x_feature = self.forward_self(x, type='x', target_token=target_token, attention=self.attention)
            x_feature = self.forward_cross(z_feature, x_feature, self.attention)

        return x_feature

    # def forward(self, z, x, **kwargs):
    #     """
    #     Joint feature extraction and relation modeling for the basic ViT backbone.
    #     Args:
    #         z (torch.Tensor): template feature, [B, C, H_z, W_z]
    #         x (torch.Tensor): search region feature, [B, C, H_x, W_x]

    #     Returns:
    #         x (torch.Tensor): merged template and search region feature, [B, L_z+L_x, C]
    #         attn : None
    #     """
    #     x, aux_dict = self.forward_features(z, x,)

    #     return x, aux_dict

    @torch.jit.ignore()
    def load_pretrained(self, checkpoint_path, prefix=''):
        _load_weights(self, checkpoint_path, prefix)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'dist_token'}

    def get_classifier(self):
        if self.dist_token is None:
            return self.head
        else:
            return self.head, self.head_dist

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        if self.num_tokens == 2:
            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()


def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):
    """ ViT weight initialization
    * When called without n, head_bias, jax_impl args it will behave exactly the same
      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl
    """
    if isinstance(module, nn.Linear):
        if name.startswith('head'):
            nn.init.zeros_(module.weight)
            nn.init.constant_(module.bias, head_bias)
        elif name.startswith('pre_logits'):
            lecun_normal_(module.weight)
            nn.init.zeros_(module.bias)
        else:
            if jax_impl:
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    if 'mlp' in name:
                        nn.init.normal_(module.bias, std=1e-6)
                    else:
                        nn.init.zeros_(module.bias)
            else:
                trunc_normal_(module.weight, std=.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    elif jax_impl and isinstance(module, nn.Conv2d):
        # NOTE conv was left to pytorch default in my original init
        lecun_normal_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):
        nn.init.zeros_(module.bias)
        nn.init.ones_(module.weight)


@torch.no_grad()
def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):
    """ Load weights from .npz checkpoints for official Google Brain Flax implementation
    """
    import numpy as np

    def _n2p(w, t=True):
        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:
            w = w.flatten()
        if t:
            if w.ndim == 4:
                w = w.transpose([3, 2, 0, 1])
            elif w.ndim == 3:
                w = w.transpose([2, 0, 1])
            elif w.ndim == 2:
                w = w.transpose([1, 0])
        return torch.from_numpy(w)

    w = np.load(checkpoint_path)
    if not prefix and 'opt/target/embedding/kernel' in w:
        prefix = 'opt/target/'

    if hasattr(model.patch_embed, 'backbone'):
        # hybrid
        backbone = model.patch_embed.backbone
        stem_only = not hasattr(backbone, 'stem')
        stem = backbone if stem_only else backbone.stem
        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))
        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))
        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))
        if not stem_only:
            for i, stage in enumerate(backbone.stages):
                for j, block in enumerate(stage.blocks):
                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'
                    for r in range(3):
                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))
                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))
                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))
                    if block.downsample is not None:
                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))
                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))
                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))
        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])
    else:
        embed_conv_w = adapt_input_conv(
            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))
    model.patch_embed.proj.weight.copy_(embed_conv_w)
    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))
    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))
    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)
    if pos_embed_w.shape != model.pos_embed.shape:
        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights
            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)
    model.pos_embed.copy_(pos_embed_w)
    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))
    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))
    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:
        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))
        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))
    if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:
        model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))
        model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))
    for i, block in enumerate(model.blocks.children()):
        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'
        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'
        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))
        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))
        block.attn.qkv.weight.copy_(torch.cat([
            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))
        block.attn.qkv.bias.copy_(torch.cat([
            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))
        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))
        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))
        for r in range(2):
            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))
            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))
        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))
        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))


def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):
    # Rescale the grid of position embeddings when loading from state_dict. Adapted from
    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224
    print('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)
    ntok_new = posemb_new.shape[1]
    if num_tokens:
        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]
        ntok_new -= num_tokens
    else:
        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]
    gs_old = int(math.sqrt(len(posemb_grid)))
    if not len(gs_new):  # backwards compatibility
        gs_new = [int(math.sqrt(ntok_new))] * 2
    assert len(gs_new) >= 2
    print('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)
    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bilinear')
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)
    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)
    return posemb


def checkpoint_filter_fn(state_dict, model):
    """ convert patch embedding weight from manual patchify + linear proj to conv"""
    out_dict = {}
    if 'model' in state_dict:
        # For deit models
        state_dict = state_dict['model']
    for k, v in state_dict.items():
        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:
            # For old models that I trained prior to conv based patchification
            O, I, H, W = model.patch_embed.proj.weight.shape
            v = v.reshape(O, -1, H, W)
        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:
            # To resize pos embedding when using model at different size from pretrained weights
            v = resize_pos_embed(
                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)
        out_dict[k] = v
    return out_dict


def _create_vision_transformer(variant, pretrained=False, default_cfg=None, **kwargs):
    if kwargs.get('features_only', None):
        raise RuntimeError('features_only not implemented for Vision Transformer models.')

    model = VisionTransformer(**kwargs)

    if pretrained:
        if 'npz' in pretrained:
            model.load_pretrained(pretrained, prefix='')
        else:
            checkpoint = torch.load(pretrained, map_location="cpu")
            missing_keys, unexpected_keys = model.load_state_dict(checkpoint["model"], strict=False)
            print('Load pretrained model from: ' + pretrained)

    return model

def _create_ViT_CAE(pretrained=False, **kwargs) -> VisionTransformer:
    model = VisionTransformer(**kwargs)

    if pretrained:
        if 'npz' in pretrained:
            model.load_pretrained(pretrained, prefix='')
        else:
            state_dict = torch.load(pretrained, map_location="cpu")
            # state_dict = torch.load(
            #     'pretrained_models/cae_base_300ep.pth', map_location="cpu")
            # new_ckpt = OrderedDict()
            state_dict = state_dict['model']
            state_dict_new = OrderedDict()
            # if sorted(list(state_dict.keys()))[0].startswith('encoder'):
            #     state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items() if k.startswith('encoder.')}
                # state_dict = {k.replace('blocks.', 'self_blocks.'): v for k, v in state_dict.items() if k.startswith('blocks.0')\
                #               or k.startswith('blocks.1.') or k.startswith('blocks.2.') or k.startswith('blocks.3.')}
                # state_dict = {k.replace('blocks.', 'cross_blocks.'): v for k, v in state_dict.items() if k.startswith('blocks.8')\
                #               or k.startswith('blocks.9.') or k.startswith('blocks.10.') or k.startswith('blocks.11.')}
            
            #0708
            for key, value in state_dict.items():
                if key.startswith('encoder.'):
                    key=key.replace('encoder.', '')
                    if key.startswith('blocks.0') or key.startswith('blocks.1.') or key.startswith('blocks.2.')\
                        or key.startswith('blocks.3.'):
                        key=key.replace('blocks.', 'self_blocks.')
                        state_dict_new[key] = value
                        # key=key.replace('self_blocks.', 'self_blocks_extreme.')
                        # state_dict_new[key] = value
                    elif key.startswith('blocks.8'):
                        key=key.replace('blocks.8.', 'cross_blocks.0.')
                        state_dict_new[key] = value
                        # key=key.replace('cross_blocks.0.', 'cross_blocks_extreme.0.')
                        # state_dict_new[key] = value
                    elif key.startswith('blocks.9'):
                        key=key.replace('blocks.9.', 'cross_blocks.1.')
                        state_dict_new[key] = value
                        # key=key.replace('cross_blocks.1.', 'cross_blocks_extreme.1.')
                        # state_dict_new[key] = value
                    elif key.startswith('blocks.10'):
                        key=key.replace('blocks.10.', 'cross_blocks.2.')
                        state_dict_new[key] = value
                        # key=key.replace('cross_blocks.2.', 'cross_blocks_extreme.2.')
                        # state_dict_new[key] = value
                    elif key.startswith('blocks.11'):
                        key=key.replace('blocks.11.', 'cross_blocks.3.')
                        state_dict_new[key] = value
                        # key=key.replace('cross_blocks.3.', 'cross_blocks_extreme.3.')
                        # state_dict_new[key] = value
                    elif key.startswith('cls_token') or key.startswith('pos_embed')\
                        or key.startswith('patch_embed') or key.startswith('norm'):
                        state_dict_new[key] = value
                #     else:
                #         state_dict_new[key] = value
                else:
                    state_dict_new[key] = value
            missing_keys, unexpected_keys = model.load_state_dict(
                state_dict_new, strict=False)
            # print("Unexpected keys:")
            # print(unexpected_keys)
            # print("Missing keys:")
            # print(missing_keys)
    return model

def vit_base_patch16_224(pretrained=False, **kwargs):
    """
    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).
    """
    model_kwargs = dict(
        patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)
    model = _create_vision_transformer('vit_base_patch16_224_in21k', pretrained=pretrained, **model_kwargs)
    return model

def CAE_Base_patch16_224_Async(pretrained=False, **kwargs) -> VisionTransformer:
    model_kwargs = dict(mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),
                        patch_size=16, embed_dim=768, num_heads=12,
                        drop_rate=0., attn_drop_rate=0., **kwargs)
    model = _create_ViT_CAE(pretrained=pretrained, **model_kwargs)
    return model